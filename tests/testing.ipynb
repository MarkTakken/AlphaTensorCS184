{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import torch\n",
    "t = utilities.Tokenizer()\n",
    "import numpy as np\n",
    "import random\n",
    "from architecture import *\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, c1, c2, causal_mask = False, N_heads = 16, w = 4, device = torch.device('cuda')):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm([c1])  # [Nx, c1]\n",
    "        self.ln2 = nn.LayerNorm([c2])  # [Ny, c2]\n",
    "        self.causal_mask = causal_mask\n",
    "        self.MAH = nn.MultiheadAttention(embed_dim=c1, kdim=c2, vdim=c2, num_heads=N_heads, batch_first=True)\n",
    "        self.ln3 = nn.LayerNorm([c1])  # [Nx, c1]\n",
    "        self.l1 = nn.Linear(c1, c1*w)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.l2 = nn.Linear(c1*w, c1)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        xn = self.ln1(x)\n",
    "        yn = self.ln2(y)\n",
    "        if self.causal_mask:\n",
    "            mask = torch.triu(torch.ones(x.shape[1], y.shape[1], dtype=bool), diagonal=1).to(self.device)\n",
    "            attn = self.MAH(xn, yn, yn, attn_mask = mask)[0]\n",
    "        else:\n",
    "            attn = self.MAH(xn, yn, yn)[0]\n",
    "        x = x + attn\n",
    "        x = x + self.l2(self.gelu(self.l1(self.ln3(x))))\n",
    "        return x\n",
    "\n",
    "class AttentiveModes(nn.Module):\n",
    "    def __init__(self, s, c):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(c, c, N_heads = 8)\n",
    "        self.s = s\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        g = [x1, x2, x3]\n",
    "        for m1, m2 in [(0, 1), (2, 0), (1, 2)]:\n",
    "            a = torch.concatenate((g[m1], torch.transpose(g[m2], 1, 2)), axis=2)\n",
    "            aflat = a.flatten(0,1)\n",
    "            c = self.attention(aflat, aflat).reshape_as(a)\n",
    "            g[m1] = c[:, :, :self.s, :]\n",
    "            g[m2] = c[:, :, self.s:, :].transpose(1,2)\n",
    "        return g\n",
    "\n",
    "\n",
    "class Torso(nn.Module):\n",
    "    def __init__(self, s, c, i):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(s, c)\n",
    "        self.attentive_modes = nn.ModuleList([AttentiveModes(s, c) for _ in range(i)])\n",
    "        self.s = s\n",
    "        self.c = c\n",
    "        self.i = i\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = torch.permute(x, (0, 1, 2, 3))\n",
    "        x2 = torch.permute(x, (0, 2, 3, 1))\n",
    "        x3 = torch.permute(x, (0, 3, 1, 2))\n",
    "\n",
    "        x1 = self.l1(x1)\n",
    "        x2 = self.l1(x2)\n",
    "        x3 = self.l1(x3)\n",
    "        \n",
    "        for am in self.attentive_modes:\n",
    "            x1, x2, x3 = am(x1, x2, x3)\n",
    "\n",
    "        e = torch.reshape(torch.stack([x1, x2, x3], axis=2), (-1, 3 * self.s ** 2, self.c))\n",
    "        return e\n",
    "\n",
    "class PolicyHead(nn.Module):\n",
    "    # Currently assumes our implemented tokenization scheme\n",
    "    # That is, Nstesp = s and Nlogits = range^3\n",
    "    def __init__(self, Nsteps, elmnt_range, s, c, Nfeatures = 64, Nheads = 16, Nlayers = 2, device = torch.device('cuda')):\n",
    "        super().__init__()\n",
    "        self.Nlayers = Nlayers\n",
    "        self.Nlogits = (elmnt_range[1]-elmnt_range[0]+1)**3\n",
    "        self.tokenizer = utilities.Tokenizer(elmnt_range)\n",
    "        self.Nsteps = Nsteps\n",
    "        self.Nfeatures = Nfeatures\n",
    "        self.Nheads = Nheads\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(self.Nlogits+1, Nfeatures * Nheads)  #In principle more efficient than forming one-hot vectors and matrix multplying\n",
    "        self.START_TOK = self.Nlogits\n",
    "        self.pos_embedding = nn.Embedding(Nsteps, Nfeatures * Nheads)\n",
    "\n",
    "        # I figure if we are keeping the weights in the LayerNorm, we might as well have\n",
    "        #   a different one for each layer, but idk really\n",
    "        self.ln1 = nn.ModuleList([nn.LayerNorm([Nfeatures * Nheads]) for _ in range(Nlayers)])  # [Nsteps, Nfeatures * Nheads]\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.self_attention = nn.ModuleList([Attention(Nfeatures * Nheads, Nfeatures * Nheads, causal_mask=True, N_heads=Nheads) for _ in range(Nlayers)])\n",
    "        self.ln2 = nn.ModuleList([nn.LayerNorm([Nfeatures * Nheads]) for _ in range(Nlayers)])\n",
    "        self.cross_attention = nn.ModuleList([Attention(Nfeatures * Nheads, c, N_heads=Nheads) for _ in range(Nlayers)])\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.lfinal = nn.Linear(Nfeatures * Nheads, self.Nlogits)\n",
    "\n",
    "    def predict_logits(self, a, e):   # Assumes a is in tokenized, not one-hot form\n",
    "        x = self.tok_embedding(a)\n",
    "        positions = torch.arange(a.shape[1]).repeat((a.shape[0], 1)).to(self.device)\n",
    "        x = x + self.pos_embedding(positions)\n",
    "        for i in range(self.Nlayers):\n",
    "            x = self.ln1[i](x)\n",
    "            c = self.self_attention[i](x, x)\n",
    "            c = self.dropout(c)  # Does not run if in evaluation mode\n",
    "            x = x + c\n",
    "            x = self.ln2[i](x)\n",
    "            c = self.cross_attention[i](x, e)\n",
    "            c = self.dropout(c)\n",
    "            x = x + c\n",
    "        o = self.lfinal(self.relu(x))\n",
    "        return o    # Don't need x bc we are not feeding it to the value head\n",
    "    \n",
    "    def forward(self, e, **kwargs):\n",
    "        if self.training:\n",
    "            g = kwargs['g']\n",
    "            #I'm not entirely sure this is right -- need to think on tokens and what the null character is\n",
    "            #g = torch.cat((torch.tensor([0]), g))\n",
    "            #Not working at the moment, going to stick with this and not shifting, but maybe there's a shift or something needed?\n",
    "            # a = nn.functional.one_hot(g, self.Nlogits).float()\n",
    "            # o, z = self.predict_logits(a, e)\n",
    "            # return o, z\n",
    "            a = torch.concatenate((torch.tensor(self.START_TOK).repeat(g.shape[0], 1).to(self.device), g[:, :-1].to(self.device)), axis=1).to(self.device)\n",
    "            return self.predict_logits(a, e)\n",
    "        \n",
    "        else:\n",
    "            Nsamples = kwargs['Nsamples']\n",
    "            #a = torch.zeros((Nsamples, self.Nsteps)).long()\n",
    "            a = [[self.START_TOK] for _ in range(Nsamples)]\n",
    "            p = torch.ones(Nsamples)\n",
    "            #z = torch.zeros((Nsamples, self.Nsteps, self.Nfeatures * self.Nheads))\n",
    "            #Don't care about exporting Z anymore\n",
    "            for j in range(Nsamples):\n",
    "                for i in range(self.Nsteps):\n",
    "                    # encoded = nn.functional.one_hot(a[j, :], self.Nlogits)\n",
    "                    # o, _ = self.predict_logits(encoded.float(), e)\n",
    "                    o = self.predict_logits(torch.tensor([a[j]]).to(self.device), e)\n",
    "                    probs = torch.softmax(o[0, i, :], -1).to('cpu')\n",
    "                    tok = torch.multinomial(probs, num_samples=1).item()\n",
    "                    a[j].append(tok)\n",
    "                    p[j] *= probs[tok]\n",
    "            \n",
    "            actions = self.tokenizer.batch_detokenize(torch.tensor(a)[:,1:])\n",
    "            probs = p/p.sum()\n",
    "\n",
    "            return namedtuple('Policy', 'actions probs')(actions, probs)\n",
    "            \n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, c, d):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.d = d\n",
    "        \n",
    "        self.l1 = nn.Linear(c, d)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(d, d)\n",
    "        self.l3 = nn.Linear(d, d)\n",
    "        self.lf = nn.Linear(d, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x, axis=1)\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.relu(self.l3(x))\n",
    "        x = self.lf(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## Before implementing heads, read up on Torch head/transformer modules and how they work further. Unclear to me if their transformers do what we want. \n",
    "## Also, need to be careful with setting up training vs acting\n",
    "\n",
    "class AlphaTensor184(nn.Module):\n",
    "    def __init__(self, s, c, d, elmnt_range, Nsteps, Nsamples, N_policy_features = 48, N_policy_heads = 12, torso_iterations = 8):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.c = c\n",
    "        self.Nlogits = elmnt_range[1]-elmnt_range[0]+1\n",
    "        self.Nsteps = Nsteps\n",
    "        self.Nsamples = Nsamples\n",
    "        \n",
    "        self.torso = Torso(s, c, torso_iterations)\n",
    "        self.value_head = ValueHead(c, d) \n",
    "        self.policy_head = PolicyHead(Nsteps, elmnt_range, s, c, Nfeatures=N_policy_features, Nheads=N_policy_heads)\n",
    "    \n",
    "    def forward(self, x, g=None):\n",
    "        e = self.torso(x)\n",
    "        q = self.value_head(e)\n",
    "        if g == None:  # Inference\n",
    "            assert(not(self.training))\n",
    "            policy = self.policy_head(e, Nsamples=self.Nsamples)\n",
    "            return (q, policy)\n",
    "        else: # Training\n",
    "            assert(self.training)\n",
    "            logits = self.policy_head(e, g=g)\n",
    "            return (q, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_modes = AttentiveModes(4, 64)\n",
    "print(attn_modes(torch.ones(64,4,4,64), torch.ones(64,4,4,64), torch.ones(64,4,4,64))[0].shape)\n",
    "torso = Torso(4,64,2)\n",
    "print(torso(torch.ones(64,4,4,4)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from architecture import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(f\"Total memory: {t}\")\n",
    "print(f\"Reserved memory: {r}\")\n",
    "print(f\"Allocated memory: {a}\")\n",
    "\n",
    "alphaTensor184 = AlphaTensor184(s = 4, c = 48, d = 48, elmnt_range=(-2, 2), N_policy_features=32, N_policy_heads=8, Nsteps=4, Nsamples=24, torso_iterations=4)\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(f\"Total memory: {t}\")\n",
    "print(f\"Reserved memory: {r}\")\n",
    "print(f\"Allocated memory: {a}\")\n",
    "\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in alphaTensor184.parameters())\n",
    "\n",
    "print(pytorch_total_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaTensor184.to(\"cuda\")\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(f\"Total memory: {t}\")\n",
    "print(f\"Reserved memory: {r}\")\n",
    "print(f\"Allocated memory: {a}\")\n",
    "\n",
    "\n",
    "alphaTensor184.train()\n",
    "\n",
    "print(alphaTensor184)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(alphaTensor184)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh = PolicyHead(10, 64, 3, 64)\n",
    "\n",
    "vh.train()\n",
    "vh.eval()\n",
    "\n",
    "vh.forward(torch.randn(3 * 3 * 3, 64), Nsamples = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 0, 0],[0,0,0], [-2, -2, -2]])\n",
    "b = torch.tensor([[-2,1,0],[0,1,1], [-1, 1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.tokenize(a)\n",
    "y = t.tokenize(b)\n",
    "print(a, b)\n",
    "print(x, y)\n",
    "w = t.detokenize(x)\n",
    "x = t.detokenize(y)\n",
    "\n",
    "print(w, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_r1(S: int, vals: list[int], factor_dist: list[float]):\n",
    "    nonzero = False\n",
    "    while not nonzero:\n",
    "        t = np.random.choice(vals, size=(3, S), p=factor_dist)\n",
    "        m = np.tensordot(np.tensordot(t[0, :], t[1, :], axes=0), t[2, :], axes=0)\n",
    "        assert m.shape == (S, S, S)\n",
    "        nonzero = np.any(m)\n",
    "    return t, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 3\n",
    "vals = [-2, -1, 0, 1, 2]\n",
    "factor_dist = [0.1, 0.2, 0.4, 0.2, 0.1]\n",
    "\n",
    "t, m = generate_sample_r1(S, vals, factor_dist)\n",
    "\n",
    "print(f\"tensors {t}\")\n",
    "print(f\"result {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(S: int, r_limit: int, factor_dist: dict, N: int, seed: int = None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    low, high = min(factor_dist.keys()), max(factor_dist.keys())\n",
    "    vals = list(factor_dist.keys())\n",
    "    dist = [factor_dist[i] for i in factor_dist.keys()]\n",
    "\n",
    "    \n",
    "    tokenizer = utilities.Tokenizer(range=(low, high))\n",
    "\n",
    "    SAR_pairs = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        R = random.randint(1, r_limit)\n",
    "        T = torch.zeros((S, S, S), dtype=torch.int)\n",
    "        reward = 0\n",
    "        for j in range(R):\n",
    "            sample, m = generate_sample_r1(S, vals, dist)\n",
    "            T += torch.from_numpy(m)\n",
    "            tokens = tokenizer.tokenize(torch.from_numpy(sample.T))\n",
    "            reward += -1\n",
    "            SAR_pairs.append((T, tokens, reward))\n",
    "\n",
    "    return SAR_pairs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"data/Sar_pairs_3_1000_123456.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main(3, 10, {-2: .1, -1 : .2, 0: 0.4, 1: 0.2, 2: 0.1}, 10, seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.batch_detokenize(torch.tensor([[0,2,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "\n",
    "class ActionDataset(Dataset):\n",
    "    def __init__(self, pregen_files, max_pregen, max_selfplay, selfplay_files = None):\n",
    "        self.max_pregen = max_pregen\n",
    "        self.max_selfplay = max_selfplay\n",
    "        l = [torch.load(file) for file in pregen_files]\n",
    "        self.pregen_actions = list(itertools.chain.from_iterable(l))[:self.max_pregen]\n",
    "        self.selfplay_actions = []\n",
    "        if selfplay_files != None:\n",
    "            l = [torch.load(file) for file in selfplay_files]\n",
    "            self.selfplay_actions = list(itertools.chain.from_iterable(l))[:self.max_selfplay]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pregen_actions) + len(self.selfplay_actions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.max_pregen:\n",
    "            return self.pregen_actions[idx]\n",
    "        else:\n",
    "            return self.selfplay_actions[idx - self.max_pregen]\n",
    "        \n",
    "    def add_selfplay_actions(self, actions):\n",
    "        self.selfplay_actions = self.selfplay_actions + actions\n",
    "        if len(self.selfplay_actions) > self.max_selfplay:\n",
    "            self.selfplay_actions = self.selfplay_actions[len(self.selfplay_actions) - self.max_selfplay:]\n",
    "\n",
    "dataset = ActionDataset([\"data/Sar_pairs_4_100000_123456.pt\"], 500000, 10000)\n",
    "\n",
    "len(dataset.pregen_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from architecture import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "def loss_fn(pred_logits, true_tokens, pred_value, true_value, val_weight=1.0, device = 'cuda'):\n",
    "    policy_loss = nn.functional.cross_entropy(pred_logits.reshape(-1, pred_logits.shape[-1]), true_tokens.flatten().type(torch.LongTensor).to(device))\n",
    "    value_loss = (torch.abs(pred_value - true_value)).mean()\n",
    "    return policy_loss + val_weight*value_loss\n",
    "\n",
    "def loss_reporter(pred_logits, true_tokens, pred_value, true_value, val_weight=1.0, device = 'cuda'):\n",
    "    policy_loss = nn.functional.cross_entropy(pred_logits.reshape(-1, pred_logits.shape[-1]), true_tokens.flatten().type(torch.LongTensor).to(device))\n",
    "    value_loss = (torch.abs(pred_value - true_value)).mean()\n",
    "    return policy_loss, value_loss\n",
    "\n",
    "def train(model, dataset, epochs, batch_size = 1024, lr=0.001, device = 'cuda'):\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        rpol, rval = 0.0, 0.0\n",
    "        for batch in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            states, actions, values = batch\n",
    "            states = states.to(device).float()\n",
    "            actions = actions.to(device)\n",
    "            values = values.to(device).float()\n",
    "            pred_value, pred_logits = model(states, g=actions)\n",
    "            loss = loss_fn(pred_logits, actions, pred_value, values, val_weight=.33)\n",
    "            pol_loss, val_loss = loss_reporter(pred_logits, actions, pred_value, values, val_weight=.33)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            rpol += pol_loss\n",
    "            rval += val_loss\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}, Policy Loss: {rpol/len(dataloader)}, Value Loss: {rval/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaTensor184.to('cuda')   \n",
    "train(alphaTensor184, dataset, 5, batch_size=1024, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self-play is meant to control the self-play loop\n",
    "from architecture import *\n",
    "from mcts import *\n",
    "from utilities import *\n",
    "from tensorgame import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Probably more convenient for state to be a tensor of ints\n",
    "#       so that we don't risk floating point inaccuracies when\n",
    "#       checking whether the state equals zero\n",
    "canonical = torch.zeros(4, 4, 4, dtype=torch.long)\n",
    "canonical[0, 0, 0] = 1\n",
    "canonical[0, 1, 1] = 1\n",
    "canonical[1, 2, 0] = 1\n",
    "canonical[1, 3, 1] = 1\n",
    "canonical[2, 0, 2] = 1\n",
    "canonical[2, 1, 3] = 1\n",
    "canonical[3, 2, 3] = 1\n",
    "canonical[3, 3, 3] = 1\n",
    "\n",
    "def self_play(model, S: int, canonical, n_plays, num_samples = 8, num_sim = 8, identifier=1, max_actions = 12,\n",
    "              cob_entries = torch.tensor([-1, 0, 1]), cob_probs = torch.tensor([.05, .9, .05]), device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    # Build a set of target tensors\n",
    "    targets = [canonical] * n_plays\n",
    "    bases_changes = [None] * n_plays\n",
    "    for i, state in enumerate(targets):\n",
    "        cob = change_of_basis(S, cob_entries, cob_probs)\n",
    "        targets[i] = apply_COB(state, cob)\n",
    "        bases_changes[i] = cob\n",
    "    # Play the game using MCTS and model for each tensor\n",
    "\n",
    "    # Not including cob in successful_trajectories anymore b/c we can just\n",
    "    #       immediately perform the reverse change of basis (see below)\n",
    "    successful_trajectories = [] # Tuples of (Initial State, [Actions], Final State)\n",
    "    SAR_pairs = [] # Tuples of (State, Action, Reward)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for i, target in tqdm(enumerate(targets)):\n",
    "        ## Need to expand this\n",
    "        # Avoding separate root = TensorGame(target, max_actions)\n",
    "        #     for clarity: We want to work with mcts.root, which is\n",
    "        #     updated with mcts.search_and_play, rather than root.\n",
    "        mcts = MCTS(TensorGame(target, max_actions), model, device=device)\n",
    "\n",
    "        # Storing all rewards; see comment below\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        for _ in range(max_actions):\n",
    "            states.append(mcts.root.state)\n",
    "            # search_and_play already calls search internally\n",
    "            r, action = mcts.search_and_play(num_samples, num_sim)\n",
    "            actions.append(action)\n",
    "            rewards.append(r)\n",
    "\n",
    "            if mcts.root.done():  # Already considers i == max_actions - 1\n",
    "                break\n",
    "        \n",
    "        rewards[-1] += mcts.root.terminal_reward()\n",
    "        \n",
    "        # I think the value should be the sum of the suffix of the list of rewards\n",
    "        #      rather than smearing the total reward equally over all actions.  For\n",
    "        #      instance, I think the last station-action pair should have reward\n",
    "        #      -1 + terminal rather than (-n + terminal)/n = -1 + terminal/n, where\n",
    "        #      n = len(actions)\n",
    "        SAR = []\n",
    "        value = 0\n",
    "        for (state, action, reward) in zip(reversed(states), reversed(actions), reversed(rewards)):\n",
    "            value += reward\n",
    "            # Maybe we could consider canonicalizing the action by sorting or smnth\n",
    "            SAR.append((state, action, value))\n",
    "\n",
    "        SAR_pairs += SAR\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if mcts.root.is_zero():\n",
    "            orig_actions = [action @ bases_changes[i] for action in actions]\n",
    "            successful_trajectories.append((target, orig_actions))\n",
    "    \n",
    "    torch.save(successful_trajectories, f\"data/successful_trajectories_{identifier}.pt\")\n",
    "    torch.save(SAR_pairs, f\"data/SAR_pairs_sp_{identifier}.pt\")\n",
    "\n",
    "    return successful_trajectories, total_reward / n_plays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture import *\n",
    "from selfplay import *  \n",
    "from time import time \n",
    "\n",
    "alphaTensor184 = AlphaTensor184(s = 4, c = 48, d = 48, elmnt_range=(-2, 2), N_policy_features=48, N_policy_heads=12, Nsteps=4, Nsamples=24, torso_iterations=4)\n",
    "alphaTensor184.to('cuda')\n",
    "alphaTensor184.load_state_dict(torch.load(\"models/model_32_4.pt\"))\n",
    "alphaTensor184.eval()\n",
    "\n",
    "random_states = []\n",
    "\n",
    "for _ in range(25):\n",
    "    random_states.append(torch.randint(1, 3, (1, 4, 4, 4),).float().to('cuda'))\n",
    "\n",
    "start = time()\n",
    "with torch.no_grad():\n",
    "    for batch in random_states:\n",
    "        alphaTensor184(batch).to('cuda')\n",
    "\n",
    "end = time()\n",
    "print(f\"Total Time: {end - start}, Avg Time: {(end - start)/25}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.train()\n",
    "model2.to('cuda')\n",
    "from utilities import *\n",
    "dataset = ActionDataset([\"data/SAR_pairs_4_100000_1.pt\"], 100000, 0)\n",
    "\n",
    "from training import *\n",
    "train(model2, dataset, 20, batch_size=1024, lr=0.02, val_weight=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
